# claude-symphony AI Benchmarking Configuration
# Model performance comparison and dynamic selection

benchmarking:
  enabled: true
  description: "AI 모델 성능 비교 및 최적 모델 동적 선택"

  # 벤치마킹이 활성화되는 스테이지
  enabled_stages:
    - "06-implementation"
    - "07-refactoring"
    - "09-testing"

  # 벤치마크 태스크 정의
  benchmark_tasks:
    code_generation:
      description: "코드 생성 품질 비교"
      models: ["claude", "codex"]
      metrics:
        - name: "correctness"
          weight: 0.4
          measure: "test_pass_rate"
        - name: "performance"
          weight: 0.2
          measure: "execution_time"
        - name: "style_compliance"
          weight: 0.2
          measure: "lint_score"
        - name: "readability"
          weight: 0.2
          measure: "complexity_score"

    refactoring:
      description: "리팩토링 품질 비교"
      models: ["codex", "claude"]
      metrics:
        - name: "complexity_reduction"
          weight: 0.3
          measure: "cyclomatic_complexity_delta"
        - name: "test_coverage"
          weight: 0.3
          measure: "coverage_percentage"
        - name: "performance_improvement"
          weight: 0.2
          measure: "benchmark_time_delta"
        - name: "maintainability"
          weight: 0.2
          measure: "maintainability_index"

    test_generation:
      description: "테스트 코드 생성 비교"
      models: ["codex", "claude"]
      metrics:
        - name: "coverage"
          weight: 0.4
          measure: "line_coverage"
        - name: "edge_cases"
          weight: 0.3
          measure: "branch_coverage"
        - name: "quality"
          weight: 0.3
          measure: "mutation_score"

  # 자동 선택 전략
  selection_strategy:
    auto: true
    method: "weighted_score"

    thresholds:
      auto_select_minimum: 0.75  # 최소 점수
      confidence_gap: 0.15       # 1위와 2위 점수 차이

    fallback:
      enabled: true
      fallback_to: "claude"
      on_tie: "prefer_faster"

  # 히스토리 추적
  history_tracking:
    enabled: true
    storage_path: "state/ai_benchmarks/"
    retention_days: 90

    track_metrics:
      - "model"
      - "task_type"
      - "scores"
      - "execution_time"
      - "token_usage"
      - "timestamp"

    aggregation:
      period: "weekly"
      metrics: ["avg_score", "success_rate", "avg_latency"]

# 동적 모델 선택 설정
dynamic_selection:
  enabled: true
  description: "태스크 특성에 따른 최적 AI 모델 자동 선택"

  criteria:
    task_type:
      weight: 0.4
      mapping:
        brainstorming: "gemini"
        implementation: "claude"
        refactoring: "codex"
        testing: "codex"
        research: "claude"

    complexity:
      weight: 0.3
      levels:
        low:
          prefer: "haiku"  # 빠른 응답
          threshold: 100   # lines of code
        medium:
          prefer: "claude"
          threshold: 500
        high:
          prefer: "claude"  # 복잡한 로직
          threshold: null

    previous_performance:
      weight: 0.3
      lookback_period: 7  # days
      min_samples: 3

  override:
    user_preference: true
    stage_assignment: true  # models.yaml의 stage_assignments 우선

# 벤치마킹 실행 설정
execution:
  parallel_benchmark: true
  max_concurrent_benchmarks: 2

  sample_tasks:
    enabled: true
    sample_size: 3
    sampling_method: "stratified"

  timeout:
    per_model: 300  # 5 minutes
    total: 900      # 15 minutes

  resource_limits:
    max_tokens_per_benchmark: 10000
    max_retries: 2

# 결과 리포팅
reporting:
  format: "markdown"
  output_path: "state/ai_benchmarks/reports/"

  include:
    - summary_table
    - metric_breakdown
    - historical_comparison
    - recommendation

  visualization:
    enabled: true
    charts:
      - "score_comparison_bar"
      - "trend_line"
      - "radar_chart"

  notifications:
    on_significant_change:
      threshold: 0.2  # 20% 변화
      channels: ["console", "handoff"]

# 통합 설정
integration:
  # HANDOFF에 벤치마크 결과 포함
  handoff:
    include_summary: true
    include_recommendation: true

  # 컨텍스트 압축 시 벤치마크 데이터 처리
  context_compression:
    preserve_recent: 3  # 최근 3개 벤치마크 결과 유지
    summarize_older: true
